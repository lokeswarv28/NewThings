{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1e7cfd-9b2c-420e-8d3d-a76f8ae89dca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe60b3df-a419-4706-b488-25c773502f77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">\n",
       "Provides utilities for leveraging secrets within notebooks.\n",
       "Databricks documentation for more info.\n",
       "    <h3></h3><b>get(scope: String, key: String): String</b> -> Gets the string representation of a secret value with scope and key<br /><b>getBytes(scope: String, key: String): byte[]</b> -> Gets the bytes representation of a secret value with scope and key<br /><b>list(scope: String): Seq</b> -> Lists secret metadata for secrets within a scope<br /><b>listScopes: Seq</b> -> Lists secret scopes<br /><br /></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.secrets.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b24dd7e-9eca-4efd-bb62-b8c7e6d770d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[SecretMetadata(key='clientid'),\n",
       " SecretMetadata(key='dbpassword'),\n",
       " SecretMetadata(key='secretvalue'),\n",
       " SecretMetadata(key='tenantid'),\n",
       " SecretMetadata(key='test')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.secrets.listScopes()\n",
    "dbutils.secrets.list(scope= 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ea900c-9777-4695-aec0-7424454cef78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "application_id = dbutils.secrets.get(scope='test', key='clientid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a52668-a86a-4cac-a4f8-3f9a0aed0192",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REDACTED]\n"
     ]
    }
   ],
   "source": [
    "print(application_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abd1e256-b546-4c66-896a-b569b628a149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## seting up mount points\n",
    "\n",
    "## 1. get the available containers in the adls gen2 for instance stg, silver, raw\n",
    "\n",
    "## 2. create a mount points for each of these containers \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4654640d-d222-4ea8-8429-15fadb5c23c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_secrets():\n",
    "    try:\n",
    "        \n",
    "        application_id = dbutils.secrets.get(scope='test', key='clientid')\n",
    "        authenticationKey = dbutils.secrets.get(scope='test', key='secretvalue')\n",
    "        tenant_id = dbutils.secrets.get(scope='test', key='tenantid')\n",
    "\n",
    "        return application_id, authenticationKey, tenant_id\n",
    "    \n",
    "    except Exception as e :\n",
    "        print(f\"An error occure in get_secrets method:: {str(e)}\")\n",
    "        raise e \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0394dc-a938-4ebd-9ed9-e5ebb0993958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## define a function , to create mount point for raw container and only for folder called \"test\" inside of it \n",
    "\n",
    "def mount_point_for_folder(adls_container_name, storage_account_name, adls_folder_name, mount_point):\n",
    "    try:\n",
    "            \n",
    "        application_id, authenticationKey, tenant_id = get_secrets()\n",
    "           \n",
    "        source = f\"abfss://{adls_container_name}@{storage_account_name}.dfs.core.windows.net/{adls_folder_name}/\"\n",
    "        print(source)\n",
    "        endpoint = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "\n",
    "        configs = {\n",
    "            \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "            \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            \"fs.azure.account.oauth2.client.id\": f\"{application_id}\",\n",
    "            \"fs.azure.account.oauth2.client.secret\": f\"{authenticationKey}\",\n",
    "            \"fs.azure.account.oauth2.client.endpoint\": f\"{endpoint}\",\n",
    "            \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"\n",
    "        }\n",
    "\n",
    "        if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "            dbutils.fs.mount(\n",
    "                source=source,\n",
    "                mount_point=mount_point,\n",
    "                extra_configs=configs\n",
    "            )\n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occure in mount_adls_folde method:: {str(e)}\")\n",
    "        raise e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46047f5-49b3-4692-b8d9-9a21026819a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abfss://raw@adlstraininggen2.dfs.core.windows.net/test/\n"
     ]
    }
   ],
   "source": [
    "adlsContainerName = 'raw'\n",
    "storageAccountName = 'adlstraininggen2'\n",
    "adls_folder_name = 'test'\n",
    "mount_point = f'/mnt/testcontainer2/{adls_folder_name}'\n",
    "\n",
    "mount_point_for_folder(adls_container_name= adlsContainerName, storage_account_name= storageAccountName, adls_folder_name= adls_folder_name, mount_point= mount_point)\n",
    "\n",
    "\n",
    "## we create a mount point to raw container only for the folder called \"test\" inside the Raw container \n",
    "\n",
    "## final steps \n",
    "\n",
    "# create the app , copy the clientid, tenantid \n",
    "# create the secrets inside the app and copy the secret value\n",
    "# in kv create the secrets for clientid, tenantid, secret value \n",
    "\n",
    "# in the IAM of Kv add role called secret office for the app\n",
    "\n",
    "# in the IAM of Storage account add role called Storage Data Contributor for the app \n",
    "\n",
    "# create the scope in the databricks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45989bed-7a16-4703-9d43-faad1221ef43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abfss://raw@adlstraininggen2.dfs.core.windows.net/\n"
     ]
    }
   ],
   "source": [
    "## mount point fot the entire Raw container\n",
    "def mount_point_for_container(adls_container_name, storage_account_name, mount_point):\n",
    "    try:\n",
    "        # Retrieve secrets\n",
    "        application_id, authenticationKey, tenant_id = get_secrets()\n",
    "\n",
    "        # Source for the entire container (without specifying folder)\n",
    "        source = f\"abfss://{adls_container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "        print(source)\n",
    "        endpoint = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "\n",
    "        # Configuration settings for OAuth authentication\n",
    "        configs = {\n",
    "            \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "            \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            \"fs.azure.account.oauth2.client.id\": f\"{application_id}\",\n",
    "            \"fs.azure.account.oauth2.client.secret\": f\"{authenticationKey}\",\n",
    "            \"fs.azure.account.oauth2.client.endpoint\": f\"{endpoint}\",\n",
    "            \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"\n",
    "        }\n",
    "\n",
    "        # Check if the mount point already exists\n",
    "        if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "            dbutils.fs.mount(\n",
    "                source=source,\n",
    "                mount_point=mount_point,\n",
    "                extra_configs=configs\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Mount point {mount_point} already exists.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in mount_point_for_container method: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Define the parameters\n",
    "adls_container_name = 'raw'\n",
    "storage_account_name = 'adlstraininggen2'\n",
    "mount_point = f'/mnt/{adls_container_name}'\n",
    "\n",
    "# Call the function to mount the entire container\n",
    "mount_point_for_container(adls_container_name=adls_container_name, storage_account_name=storage_account_name, mount_point=mount_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13fbbded-247c-4c4c-b06e-a3a02acf055b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/testcontainer/raw\n"
     ]
    }
   ],
   "source": [
    "print(mount_pointt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b448dad-fdc9-49a5-9fd9-8766bd1e8567",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/FileStore/tables/', name='tables/', size=0, modificationTime=1729399552000)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/')\n",
    "#dbfs:/FileStore/ >> /mnt/Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac2437f-4b0c-4c01-aea7-df05cf6f9544",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\n",
       "this package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\n",
       "another FileSystem URI.\n",
       "\n",
       "For more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n",
       "\n",
       "In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\n",
       "straightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\n",
       "translates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n",
       "    <h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /><h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a01969-2e80-4c1d-8ef7-91db9f926e95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),\n",
       " MountInfo(mountPoint='/mnt/RawContainer', source='abfss://raw@adlstraininggen2.dfs.core.windows.net/raw/', encryptionType=''),\n",
       " MountInfo(mountPoint='/mnt/stg', source='abfss://stg@adlstraininggen2.dfs.core.windows.net/stg/', encryptionType=''),\n",
       " MountInfo(mountPoint='/Volumes', source='UnityCatalogVolumes', encryptionType=''),\n",
       " MountInfo(mountPoint='/databricks/mlflow-tracking', source='databricks/mlflow-tracking', encryptionType=''),\n",
       " MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType=''),\n",
       " MountInfo(mountPoint='/mnt/raw', source='abfss://raw@adlstraininggen2.dfs.core.windows.net/', encryptionType=''),\n",
       " MountInfo(mountPoint='/databricks/mlflow-registry', source='databricks/mlflow-registry', encryptionType=''),\n",
       " MountInfo(mountPoint='/mnt/testcontainer/raw', source='abfss://raw@adlstraininggen2.dfs.core.windows.net/raw/', encryptionType=''),\n",
       " MountInfo(mountPoint='/Volume', source='DbfsReserved', encryptionType=''),\n",
       " MountInfo(mountPoint='/mnt/testcontainer2/test', source='abfss://raw@adlstraininggen2.dfs.core.windows.net/test/', encryptionType=''),\n",
       " MountInfo(mountPoint='/mnt/stagecontainer', source='abfss://stg@adlstraininggen2.dfs.core.windows.net/stg/', encryptionType=''),\n",
       " MountInfo(mountPoint='/volumes', source='DbfsReserved', encryptionType=''),\n",
       " MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType=''),\n",
       " MountInfo(mountPoint='/volume', source='DbfsReserved', encryptionType='')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb22860-9abc-4537-a30e-3354078350ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/testcontainer2/test/Pyspark_oct_19.txt', name='Pyspark_oct_19.txt', size=6052, modificationTime=1729402374000),\n",
       " FileInfo(path='dbfs:/mnt/testcontainer2/test/Pyspark_oct_6.txt', name='Pyspark_oct_6.txt', size=2294, modificationTime=1729402374000)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls('/mnt/testcontainer2/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415a1dd5-57b0-48dc-be6c-a5ab615df03f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/raw/2-WheelSales.csv', name='2-WheelSales.csv', size=2085500, modificationTime=1720248338000),\n",
       " FileInfo(path='dbfs:/mnt/raw/2_wheels_mapping.csv', name='2_wheels_mapping.csv', size=2032342, modificationTime=1718431607000),\n",
       " FileInfo(path='dbfs:/mnt/raw/4-WheelSales.csv', name='4-WheelSales.csv', size=10992762, modificationTime=1720248369000),\n",
       " FileInfo(path='dbfs:/mnt/raw/API/', name='API/', size=0, modificationTime=1719121722000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Book1_01.csv', name='Book1_01.csv', size=70, modificationTime=1725164419000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Events/', name='Events/', size=0, modificationTime=1722662144000),\n",
       " FileInfo(path='dbfs:/mnt/raw/IncrementalOutput/', name='IncrementalOutput/', size=0, modificationTime=1717307858000),\n",
       " FileInfo(path='dbfs:/mnt/raw/LOGS/', name='LOGS/', size=0, modificationTime=1716617717000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Logs2/', name='Logs2/', size=0, modificationTime=1716698955000),\n",
       " FileInfo(path='dbfs:/mnt/raw/MOCK_DATA.csv', name='MOCK_DATA.csv', size=5953, modificationTime=1722661791000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Metacolumns.csv', name='Metacolumns.csv', size=1915, modificationTime=1720248390000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Orders/', name='Orders/', size=0, modificationTime=1716703610000),\n",
       " FileInfo(path='dbfs:/mnt/raw/People/', name='People/', size=0, modificationTime=1716703649000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Returns/', name='Returns/', size=0, modificationTime=1716703631000),\n",
       " FileInfo(path='dbfs:/mnt/raw/SchemaManagementDelta.csv', name='SchemaManagementDelta.csv', size=101806, modificationTime=1716617971000),\n",
       " FileInfo(path='dbfs:/mnt/raw/SchemaMoreCols.zip/', name='SchemaMoreCols.zip/', size=0, modificationTime=1716013579000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Share SourceTable_data_preview.csv', name='Share SourceTable_data_preview.csv', size=224727, modificationTime=1725791564000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Split Column Range to Rows.csv', name='Split Column Range to Rows.csv', size=98, modificationTime=1719634651000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Table_data_preview.csv', name='Table_data_preview.csv', size=224727, modificationTime=1725791653000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Year=2024/', name='Year=2024/', size=0, modificationTime=1716101152000),\n",
       " FileInfo(path='dbfs:/mnt/raw/Zoho_people_code.csv', name='Zoho_people_code.csv', size=86, modificationTime=1724475972000),\n",
       " FileInfo(path='dbfs:/mnt/raw/code_zoho.csv', name='code_zoho.csv', size=70, modificationTime=1723897659000),\n",
       " FileInfo(path='dbfs:/mnt/raw/data_aug24.json', name='data_aug24.json', size=49160, modificationTime=1724476142000),\n",
       " FileInfo(path='dbfs:/mnt/raw/dept_data.csv', name='dept_data.csv', size=63, modificationTime=1725790422000),\n",
       " FileInfo(path='dbfs:/mnt/raw/employee_data.csv', name='employee_data.csv', size=374, modificationTime=1725790422000),\n",
       " FileInfo(path='dbfs:/mnt/raw/fb_data2.json', name='fb_data2.json', size=17356, modificationTime=1720935787000),\n",
       " FileInfo(path='dbfs:/mnt/raw/holidays.csv', name='holidays.csv', size=58, modificationTime=1719030338000),\n",
       " FileInfo(path='dbfs:/mnt/raw/sales_07_data.csv', name='sales_07_data.csv', size=294, modificationTime=1725790422000),\n",
       " FileInfo(path='dbfs:/mnt/raw/test/', name='test/', size=0, modificationTime=1729402358000)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls('/mnt/raw/')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Nb_SetUp_Mount_point",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
