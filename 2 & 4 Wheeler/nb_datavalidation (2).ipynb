{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5eb7674-5ee6-4c15-8e39-e1c7cc131e44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dfe4382-77d1-42b1-83bf-66e8abab42d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dt_string = datetime.now().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "metadata_path = 'dbfs:/Metadata/Metacolumns__1_.csv'\n",
    "Metadata = spark.read.csv(metadata_path, header='True')\n",
    "df1_path = f\"dbfs:/Bronze/2_WheelSales/{dt_string}\"\n",
    "df2_path = f\"dbfs:/Bronze/4_WheelSales/{dt_string}\"\n",
    "parquet_path_df1 = 'dbfs:/Silver'\n",
    "parquet_path_df2 = 'dbfs:/Silver'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f927a30-568e-4bbc-80a8-add4b9400d54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------------+-----------------+------------+-----------------+--------------+--------------+--------------+\n|  ID|Schema|         TableName|       ColumnName|    FILENAME|      FILECOLUMNS| RENAMECOLUMNS|SourceDataType|TargetDataType|\n+----+------+------------------+-----------------+------------+-----------------+--------------+--------------+--------------+\n|   1|   Dim|             Brand|            Brand|2-WheelSales|              OEM|         brand|        string|        string|\n|   3|   Dim|              Fuel|         FuelType|2-WheelSales|            Model|          null|        string|        string|\n|   4|   Dim|          Location|          Country|2-WheelSales|          Segment|          null|        string|        string|\n|   4|   Dim|          Location|           Region|2-WheelSales|            Month|   monthofsale|        string|        string|\n|   4|   Dim|          Location|             City|2-WheelSales|             Year|    yearofsale|           int|           int|\n|   5|   Dim|             Model|            Model|2-WheelSales|        NoOfSales|          null|           int|           int|\n|   5|   Dim|             Model|          Segment|2-WheelSales|   EngineCapacity|          null|        string|        string|\n|   5|   Dim|             Model|  fuelcapacity(l)|2-WheelSales|          Mileage|          null|        string|        string|\n|   5|   Dim|             Model|         BodyType|2-WheelSales|      Model Price|    pricerange|        string|        string|\n|   5|   Dim|             Model|  SeatingCapacity|2-WheelSales|  Dealer location|          null|        string|        string|\n|   6|   Dim|      Transmission|     Transmission|2-WheelSales|           Region|          null|        string|        string|\n|   7|  Fact|SalesOfFourWheeler|   EngineCapacity|4-WheelSales|            Brand|          null|        string|        string|\n|   7|  Fact|SalesOfFourWheeler|          Mileage|4-WheelSales|            Model|          null|        string|        string|\n|   7|  Fact|SalesOfFourWheeler|Top_Speed in Km/h|4-WheelSales|        Body Type|       segment|        string|        string|\n|   7|  Fact|SalesOfFourWheeler|    NoOfCylinders|4-WheelSales|        FUEL TYPE|          null|        string|        string|\n|   7|  Fact|SalesOfFourWheeler|       Pricerange|4-WheelSales|          Mileage|          null|        string|        string|\n|   7|  Fact|SalesOfFourWheeler|       yearofsale|4-WheelSales|           ENGINE|enginecapacity|        string|        string|\n|   7|  Fact|SalesOfFourWheeler|      monthofsale|4-WheelSales|     TRANSMISSION|          null|        string|        string|\n|   8|  Fact| SalesOfTwoWheeler|   EngineCapacity|4-WheelSales|      Price range|          null|        string|        string|\n|   8|  Fact| SalesOfTwoWheeler|          Mileage|4-WheelSales|Fuel Capacity (L)|          null|           int|           int|\n|   8|  Fact| SalesOfTwoWheeler|        NoOfSales|4-WheelSales| Seating Capacity|          null|           int|           int|\n|   8|  Fact| SalesOfTwoWheeler|       Pricerange|4-WheelSales|Top_Speed in Km/h|          null|        string|        string|\n|   8|  Fact| SalesOfTwoWheeler|       yearofsale|4-WheelSales|  No Of Cylinders|          null|           int|           int|\n|   8|  Fact| SalesOfTwoWheeler|      monthofsale|4-WheelSales|     Year Of Sale|          null|           int|           int|\n|null|  null|              null|             null|4-WheelSales|    Month Of Sale|          null|        string|        string|\n|null|  null|              null|             null|4-WheelSales|          Country|          null|        string|        string|\n|null|  null|              null|             null|4-WheelSales|             City|          null|        string|        string|\n+----+------+------------------+-----------------+------------+-----------------+--------------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "ref_df = spark.read.format('csv').option('Header', True).option('inferSchema', True).load('dbfs:/Metadata/Metacolumns__1_.csv').withColumn('FILECOLUMNS', trim('FILECOLUMNS')).show(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27191971-3b9a-4194-942c-06a673402dc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def column_comparision(srcpath, filename, parquet_path):\n",
    "    print(\"reading the source file\")\n",
    "    \n",
    "    dfs = spark.read.format('csv').option('Header', True).option('inferSchema', True).load(srcpath)\n",
    "    \n",
    "    print(\"reading metacolumns file\")\n",
    "\n",
    "    ref_df = spark.read.format('csv').option('Header', True).option('inferSchema', True).load('dbfs:/Metadata/Metacolumns__1_.csv').withColumn('FILECOLUMNS', trim('FILECOLUMNS'))\n",
    "\n",
    "    df_columns = set(map(str.lower, dfs.columns))\n",
    "    print(\"DF Columns:\", df_columns)\n",
    "\n",
    "    ref_filter = ref_df.filter(col('FILENAME') == filename)\n",
    "\n",
    "    ref_columns = set(map(str.lower, ref_filter.select('FILECOLUMNS').rdd.flatMap(lambda x: x).collect()))\n",
    "    print(\"Ref Columns:\", ref_columns)\n",
    "\n",
    "    # Compare column orders\n",
    "    if df_columns == ref_columns:\n",
    "        print(\"Column orders match. and writing to parquet path\")\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m/%d\")\n",
    "    \n",
    "        dfs.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").format('parquet').save(f\"{parquet_path}/{filename}/{timestamp}\")\n",
    "        print(\"done written to parquet file...\")\n",
    "\n",
    "    else:\n",
    "        print(\"Column orders differ:\")\n",
    "        missmatched_cols = ref_columns - df_columns\n",
    "        print(missmatched_cols)\n",
    "        error_message = \"Column names don't match for {} and missing columns are {}: \".format(filename,)\n",
    "        print(\"DF columns:\", df_columns)\n",
    "        print(\"Ref columns:\", ref_columns)\n",
    "\n",
    "        raise ValueError(error_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb7e506-9ffb-4a17-bd04-6f04cf30ca0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the source file\nreading metacolumns file\nDF Columns: {'year', 'region', 'segment', 'enginecapacity', 'mileage', 'model', 'model price', 'dealer location', 'oem', 'month', 'noofsales'}\nRef Columns: {'year', 'region', 'segment', 'enginecapacity', 'mileage', 'model', 'model price', 'dealer location', 'oem', 'month', 'noofsales'}\nColumn orders match. and writing to parquet path\ndone written to parquet file...\nreading the source file\nreading metacolumns file\nDF Columns: {'no of cylinders', 'body type', 'month of sale', 'fuel capacity (l)', 'mileage', 'brand', 'country', 'model', 'city', 'transmission', 'engine', 'price range', 'seating capacity', 'year of sale', 'fuel type', 'top_speed in km/h'}\nRef Columns: {'no of cylinders', 'body type', 'month of sale', 'fuel capacity (l)', 'mileage', 'brand', 'country', 'model', 'city', 'transmission', 'engine', 'price range', 'seating capacity', 'year of sale', 'fuel type', 'top_speed in km/h'}\nColumn orders match. and writing to parquet path\ndone written to parquet file...\n"
     ]
    }
   ],
   "source": [
    "column_comparision(df1_path, '2-WheelSales', parquet_path_df1)\n",
    "column_comparision(f\"dbfs:/Bronze/4_WheelSales/{dt_string}\", '4-WheelSales', parquet_path_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0d0fbf-f3fd-4539-bc49-8e06deb62486",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# column_comparision(srcpath= 'dbfs:/FileStore/2-WheelSales/2_WheelSales.csv', filename='2-WheelSales', parquet_path = 'dbfs:/project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6154a4a-bc05-4c1c-a082-42f528b86c0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# meta_path = 'dbfs:/FileStore/Metacolumns-2.csv'\n",
    "\n",
    "def schema_comparision(srcpath, filename, parquet_path):\n",
    "    print(\"reading the source file\")\n",
    "    \n",
    "    dfs = spark.read.format('csv').option('Header', True).option('inferSchema', True).load(srcpath)\n",
    "    \n",
    "    print(\"reading metacolumns file\")\n",
    "\n",
    "    # ref_df = spark.read.format('csv').option('Header', True).option('inferSchema', True).load('dbfs:/Metadata/Metacolumns__1_.csv')\n",
    "    ref_df = spark.read.format('csv').option('Header', True).option('inferSchema', True).load('dbfs:/Metadata/Metacolumns__1_.csv').withColumn('FILECOLUMNS', trim('FILECOLUMNS'))\n",
    "\n",
    "    ref_filter = ref_df.filter(col('FILENAME') == filename)\n",
    " \n",
    "    for x in ref_filter.collect():\n",
    "        columnNames = x['FILECOLUMNS']\n",
    "        # print(columnNames)\n",
    "        refTypes = x['SourceDataType']\n",
    "        # print(refTypes)\n",
    "\n",
    "        columnNamesList = [x.strip().lower() for x in columnNames.split(\",\")]\n",
    "                \n",
    "        refTypesList = [x.strip().lower() for x in refTypes.split(\",\")]\n",
    "        #print(refTypesList)\n",
    "\n",
    "        dfsTypes = dfs.schema[columnNames].dataType.simpleString() #StringType() : string , IntergerType() : int\n",
    "        \n",
    "        dfsTypesList = [x.strip().lower() for x in dfsTypes.split(\",\")]\n",
    "        \n",
    "        # columnName : Row id, DataFrameType : int, reftype: int\n",
    "        \n",
    "        missmatchedcolumns = [(col_name, df_types, ref_types) for (col_name, df_types, ref_types) in zip(columnNamesList, dfsTypesList, refTypesList) if dfsTypesList != refTypesList]\n",
    "\n",
    "        if missmatchedcolumns :\n",
    "            error_msg = \"Schema mismatch for {} in the following columns:\".format(filename)\n",
    "            print(\"schema comparision has been failed or missmatched for this {}\".format(filename))\n",
    "            \n",
    "            for col_name, df_types, ref_types in missmatchedcolumns:\n",
    "                print(f\"columnName : {col_name}, DataFrameType : {df_types}, referenceType : {ref_types}\")\n",
    "            \n",
    "            raise ValueError(error_msg)\n",
    "            # error_message = \"Column names don't match for {} and missing columns are: {}\".format(filename, missing_columns)\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            print(\"Schema comaprision is done and success for {}\".format(filename))\n",
    "            print(\"writing to the parquet path...\")\n",
    "            timestamp = datetime.now().strftime(\"%Y%m/%d\")\n",
    "            dfs.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").format('parquet').save(f\"{parquet_path}/{filename}/{timestamp}\")\n",
    "            print(\"done written to parquet file...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c588b6-69d5-457c-bc2d-dd7204904ac4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the source file\nreading metacolumns file\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 2-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nreading the source file\nreading metacolumns file\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\nSchema comaprision is done and success for 4-WheelSales\nwriting to the parquet path...\ndone written to parquet file...\n"
     ]
    }
   ],
   "source": [
    "schema_comparision(df1_path, '2-WheelSales', parquet_path_df1)\n",
    "schema_comparision(df2_path, '4-WheelSales', parquet_path_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7004756-0001-4302-b83d-507fbb456487",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fill_nulls_duplicates(srcpath, filename, parquet_path):\n",
    "\n",
    "    print(\"reading the source file\")\n",
    "    \n",
    "    df = spark.read.format('csv').option('Header', True).option('inferSchema', True).load(srcpath)\n",
    "    \n",
    "    print(\"reading metacolumns file\")\n",
    "\n",
    "    # ref_df = spark.read.format('csv').option('Header', True).option('inferSchema', True).load('dbfs:/FileStore/Metacolumns-1.csv')\n",
    "    ref_df = spark.read.format('csv').option('Header', True).option('inferSchema', True).load('dbfs:/Metadata/Metacolumns__1_.csv').withColumn('FILECOLUMNS', trim('FILECOLUMNS'))\n",
    "\n",
    "    # Metadata = ref_df.filter((F.col('FILENAME') == filename))\n",
    "\n",
    "    # total = len(df.columns) + Metadata.select('FILECOLUMNS').count()\n",
    "    \n",
    "    # Metadata = Metadata.withColumn(\"ColumnName\", lower(col(\"ColumnName\")))\n",
    "    # Metadata = Metadata.withColumn(\"SourceDataType\", lower(col(\"SourceDataType\")))\n",
    "    # Metadata = Metadata.withColumn(\"TargetDataType\", lower(col(\"TargetDataType\")))\n",
    "    \n",
    "    for col_name, data_type in zip(df.columns, df.dtypes):\n",
    "        if data_type[1] == 'string':\n",
    "            df = df.fillna('unknown', subset=[col_name])\n",
    "        elif data_type[1] == 'int':\n",
    "            df = df.fillna(0, subset=[col_name])\n",
    "\n",
    "    null_counts = {col_name: df.filter(col(col_name).isNull()).count() for col_name in df.columns}\n",
    "    for col_name, count in null_counts.items():\n",
    "        print(f\"Null count in {col_name}: {count}\")\n",
    "\n",
    "    print(\"initial df count:\", df.count())\n",
    "    count_duplicates_i = df.groupBy(df.columns).count().filter(\"count>=1\")\n",
    "    count_duplicates_i.show()\n",
    "    print('count_duplicates_i', count_duplicates_i.count())\n",
    "    df = df.dropDuplicates()\n",
    "    count_duplicates = df.groupBy(df.columns).count().filter(\"count>1\")\n",
    "    print('Duplicate rows count:')\n",
    "    count_duplicates.show()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m/%d\")\n",
    "    \n",
    "    df.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").format('parquet').save(f\"{parquet_path}/{filename}/{timestamp}\")\n",
    "\n",
    "    print(\"return to parquet done...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a76227d3-85b1-44aa-8e1a-3db534b04ba1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the source file\nreading metacolumns file\nNull count in OEM: 0\nNull count in Model: 0\nNull count in Segment: 0\nNull count in Month: 0\nNull count in Year: 0\nNull count in NoOfSales: 0\nNull count in EngineCapacity: 0\nNull count in Mileage: 0\nNull count in Model Price: 0\nNull count in Dealer location: 0\nNull count in Region: 0\ninitial df count: 25346\n+-------------+---------------+-----------+-----+----+---------+--------------+----------+-------------------+---------------+------+-----+\n|          OEM|          Model|    Segment|Month|Year|NoOfSales|EngineCapacity|   Mileage|        Model Price|Dealer location|Region|count|\n+-------------+---------------+-----------+-----+----+---------+--------------+----------+-------------------+---------------+------+-----+\n|Royal Enfield|         Bullet|Performance|  Aug|2022|     7618|           350|     35-40|1,56,800 - 1,85,800|          Delhi| North|    1|\n|        Honda|           Livo| Motorcycle|  Dec|2022|     4587|        109.51|        74|    58,815 - 62,964|          Delhi| North|    1|\n|        Honda|        X Blade| Motorcycle|  Dec|2022|      373|         162.7|        50|1,08,443 - 1,14,402|          Delhi| North|    1|\n|        Honda|       CBR 650F|Performance|  May|2021|       13|           649|        20|             888045|          Delhi| North|    1|\n|        Honda|         GL1800|Performance|  Mar|2022|        0|          1833|        17|            2776512|          Delhi| North|    1|\n|        Honda|     CB UNICORN| Motorcycle|  Feb|2022|    12812|         149.2|        60|              97356|          Delhi| North|    1|\n|Hero MotoCorp|        Passion|      Motor|  Mar|2022|     6751|         110cc|   60 kmpl|         Rs. 69,950|          Delhi| North|    2|\n|        Bajaj|         Pulsar| Motorcycle|  Mar|2022|     2531|           220|     30-35|             128508|         Jaipur| North|    1|\n|       Yamaha|             R3|performance|  dec|2021|       15|           320|     20-25|             489900|         Jaipur| North|    1|\n|       Yamaha|          MT 15| Motorcycle|  Jun|2022|     7785|           155|     40-45|1,45,300 - 1,50,300|         Jaipur| North|    1|\n|        Honda|          Dream| Motorcycle|  Aug|2021|    11743|        109.51|        74|    50,834 - 55,324|         Jaipur| North|    1|\n|        Honda|     CB Unicorn| Motorcycle|  May|2021|     1320|         162.7|        60|             101700|         Jaipur| North|    1|\n|          TVS|XL Super- MOPED|      Moped|  Sep|2021|    61664|       69.9 cc|   67 kmpl|         Rs. 40,804|         Jaipur| North|    1|\n|      Piaggio|     Aprilia SR|    Scooter|  Aug|2021|        0|         154cc|37.88 kmpl|       Rs. 1,08,974|         Jaipur| North|    1|\n|      Okinawa|           Lite| EV Scooter|  Jul|2021|      251|       unknown|   50 kmpl|         Rs. 63,990|         Jaipur| North|    1|\n|        Bajaj|         Pulsar| Motorcycle|  Oct|2022|    18138|           150|     45-50|  98,835 - 1,04,314|     Chandigarh| North|    1|\n|        Honda|         GRAZIA|    Scooter|  Aug|2022|        0|           124|        55|    76,823 - 83,391|     Chandigarh| North|    1|\n|          TVS|           Zest|    Scooter|  Sep|2022|     5913|      109.7 cc|   64 kmpl|         Rs. 61,425|     Chandigarh| North|    1|\n|          TVS|          Ntorq|    Scooter|  Jul|2022|    24367|      124.8 cc|51.54 kmpl|         Rs. 80,040|     Chandigarh| North|    1|\n|          TVS|          Ntorq|    Scooter|  Jan|2022|    21120|      124.8 cc|51.54 kmpl|         Rs. 80,040|     Chandigarh| North|    1|\n+-------------+---------------+-----------+-----+----+---------+--------------+----------+-------------------+---------------+------+-----+\nonly showing top 20 rows\n\ncount_duplicates_i 23427\nDuplicate rows count:\n+---+-----+-------+-----+----+---------+--------------+-------+-----------+---------------+------+-----+\n|OEM|Model|Segment|Month|Year|NoOfSales|EngineCapacity|Mileage|Model Price|Dealer location|Region|count|\n+---+-----+-------+-----+----+---------+--------------+-------+-----------+---------------+------+-----+\n+---+-----+-------+-----+----+---------+--------------+-------+-----------+---------------+------+-----+\n\nreturn to parquet done...\nreading the source file\nreading metacolumns file\nNull count in Brand: 0\nNull count in Model: 0\nNull count in Body Type: 0\nNull count in FUEL TYPE: 0\nNull count in Mileage: 0\nNull count in ENGINE: 0\nNull count in TRANSMISSION: 0\nNull count in Price range: 0\nNull count in Fuel Capacity (L): 0\nNull count in Seating Capacity: 0\nNull count in Top_Speed in Km/h: 0\nNull count in No Of Cylinders: 0\nNull count in Year Of Sale: 0\nNull count in Month Of Sale: 0\nNull count in Country: 0\nNull count in City: 0\ninitial df count: 91440\n+-------+-------------+---------+---------+-----------+---------------+------------+--------------------+-----------------+----------------+-----------------+---------------+------------+-------------+-------+---------+-----+\n|  Brand|        Model|Body Type|FUEL TYPE|    Mileage|         ENGINE|TRANSMISSION|         Price range|Fuel Capacity (L)|Seating Capacity|Top_Speed in Km/h|No Of Cylinders|Year Of Sale|Month Of Sale|Country|     City|count|\n+-------+-------------+---------+---------+-----------+---------------+------------+--------------------+-----------------+----------------+-----------------+---------------+------------+-------------+-------+---------+-----+\n|Porsche|          911|    Coupe|   Petrol|9 - 12 kmpl|2981 to 3745 cc|   Automatic|   1.64 - 3.08 Crore|               64|               4|              330|              8|        2018|      October|  India|Hyderabad|    1|\n|Porsche|          911|    Coupe|   Petrol|9 - 12 kmpl|2981 to 3745 cc|      Manual|   1.64 - 3.08 Crore|               64|               4|              330|              8|        2019|    September|  India|    Delhi|    1|\n|Porsche|          911|    Coupe|   Petrol|9 - 12 kmpl|2981 to 3745 cc|      Manual|   1.64 - 3.08 Crore|               64|               4|              330|              8|        2017|       August|  India|  Kolkata|    1|\n|Porsche|        Macan|      SUV|   Petrol|    14 kmpl|1984 to 2995 cc|   Automatic|  69.98 - 85.01 Lakh|               75|               5|              254|              6|        2018|      January|  India|  Kolkata|    1|\n|Porsche|Cayenne Coupe|    Coupe|   Petrol| 10.75 kmpl|2995 to 3996 cc|   Automatic|   1.32 - 1.98 Crore|               75|               4|              286|              8|        2017|       August|  India|     Pune|    1|\n|Porsche|          911|    Coupe|   Petrol|9 - 12 kmpl|2981 to 3745 cc|      Manual|   1.64 - 3.08 Crore|               64|               4|              330|              8|        2019|      October|  India|   Jaipur|    1|\n|Porsche|          718|    Coupe|   Petrol|     9 kmpl|1988 to 3995 cc|   Automatic|85.46 Lakh - 1.64...|               54|               2|              275|              8|        2019|    September|  India|  Chennai|    1|\n|Porsche|Cayenne Coupe|    Coupe|   Petrol| 10.75 kmpl|2995 to 3996 cc|   Automatic|   1.32 - 1.98 Crore|               75|               4|              286|              8|        2019|      October|  India|  Lucknow|    1|\n|   Audi|           A6|    Sedan|   Petrol|    14 kmpl|        1984 cc|   Automatic|  55.86 - 60.51 Lakh|               73|               5|              250|              6|        2018|     November|  India|   Mumbai|    1|\n|   Audi|           A4|    Sedan|   Petrol|  12.3 kmpl|        1984 cc|   Automatic|  42.34 - 46.67 Lakh|               54|               5|              241|              6|        2018|       August|  India|Ahmedabad|    1|\n|   Audi|        RS Q8|      SUV|   Petrol|     8 kmpl|        3996 cc|   Automatic|          2.07 Crore|               85|               5|              305|              6|        2018|      October|  India|Ahmedabad|    1|\n|   Audi|         A8 L|    Sedan|   Petrol|    12 kmpl|        2995 cc|   Automatic|          1.58 Crore|               82|               4|              250|              6|        2019|     November|  India|Ahmedabad|    1|\n|   Audi| S5 Sportback|    Coupe|   Petrol|    11 kmpl|        2995 cc|   Automatic|          79.06 Lakh|               58|               5|              250|              8|        2017|       August|  India|    Delhi|    1|\n|   Audi| S5 Sportback|    Coupe|   Petrol|    11 kmpl|        2995 cc|   Automatic|          79.06 Lakh|               58|               5|              250|              8|        2017|      January|  India|Bangalore|    1|\n|   Audi| S5 Sportback|    Coupe|   Petrol|    11 kmpl|        2995 cc|   Automatic|          79.06 Lakh|               58|               5|              250|              8|        2017|      October|  India|Bangalore|    1|\n|   Audi|RS7 Sportback|    Coupe|   Petrol|     9 kmpl|        3996 cc|   Automatic|          1.94 Crore|               73|               4|              250|              8|        2018|      January|  India|Bangalore|    1|\n|   Audi|         A8 L|    Sedan|   Petrol|    12 kmpl|        2995 cc|   Automatic|          1.58 Crore|               82|               4|              250|              6|        2017|          May|  India|  Chennai|    1|\n|   Audi| S5 Sportback|    Coupe|   Petrol|    11 kmpl|        2995 cc|   Automatic|          79.06 Lakh|               58|               5|              250|              8|        2019|       August|  India|  Chennai|    1|\n|   Audi|         A8 L|    Sedan|   Petrol|    12 kmpl|        2995 cc|   Automatic|          1.58 Crore|               82|               4|              250|              6|        2017|     December|  India|  Kolkata|    1|\n|   Audi| S5 Sportback|    Coupe|   Petrol|    11 kmpl|        2995 cc|   Automatic|          79.06 Lakh|               58|               5|              250|              8|        2019|     December|  India|  Kolkata|    1|\n+-------+-------------+---------+---------+-----------+---------------+------------+--------------------+-----------------+----------------+-----------------+---------------+------------+-------------+-------+---------+-----+\nonly showing top 20 rows\n\ncount_duplicates_i 91440\nDuplicate rows count:\n+-----+-----+---------+---------+-------+------+------------+-----------+-----------------+----------------+-----------------+---------------+------------+-------------+-------+----+-----+\n|Brand|Model|Body Type|FUEL TYPE|Mileage|ENGINE|TRANSMISSION|Price range|Fuel Capacity (L)|Seating Capacity|Top_Speed in Km/h|No Of Cylinders|Year Of Sale|Month Of Sale|Country|City|count|\n+-----+-----+---------+---------+-------+------+------------+-----------+-----------------+----------------+-----------------+---------------+------------+-------------+-------+----+-----+\n+-----+-----+---------+---------+-------+------+------------+-----------+-----------------+----------------+-----------------+---------------+------------+-------------+-------+----+-----+\n\nreturn to parquet done...\n"
     ]
    }
   ],
   "source": [
    "fill_nulls_duplicates(df1_path, '2-WheelSales', parquet_path_df1)\n",
    "fill_nulls_duplicates(df2_path, '4-WheelSales', parquet_path_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90478fa-5df1-4759-9990-bb647f68352c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38bc673b-be66-4b55-98c0-c2f61fb2f50a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_and_rename(srcpath, metapath, filename, parquet_path):\n",
    "    \"\"\"\n",
    "    srcpath : source_filepath, \n",
    "    metapath : path of metacolumns file \n",
    "    filename : is used to filter Rename and filecolumns according to file\n",
    "    \"\"\"\n",
    "    print(\"reading the source file\")\n",
    "    df = spark.read.format('csv').option('Header', True).option('inferSchema', True).load(srcpath)\n",
    "    print(\"reading metacolumns file\")\n",
    "    ref_df = spark.read.format('csv').option('Header', True).option('inferSchema', True).load(metapath)\n",
    "\n",
    "    reference_df = ref_df.filter((F.col('FILENAME') == filename) & (F.col('RENAMECOLUMNS')!=' ')).select('FILECOLUMNS', 'RENAMECOLUMNS')\n",
    "    print(\"define a mapping condition\")\n",
    "    mappings = {row['FILECOLUMNS']: row['RENAMECOLUMNS'] for row in reference_df.collect()}\n",
    "\n",
    "    for original_col, new_col in mappings.items():\n",
    "        df = df.withColumnRenamed(original_col, new_col)\n",
    "        print(\"done\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m/%d\")\n",
    "    \n",
    "    df.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").format('parquet').save(f\"{parquet_path}/{filename}/{timestamp}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b988b60f-eff5-4869-bd9c-30976017e511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the source file\nreading metacolumns file\ndefine a mapping condition\ndone\ndone\ndone\ndone\nreading the source file\nreading metacolumns file\ndefine a mapping condition\ndone\ndone\n"
     ]
    }
   ],
   "source": [
    "df1 = read_and_rename(df1_path, 'dbfs:/Metadata/Metacolumns__1_.csv', '2-WheelSales',parquet_path_df1)\n",
    "df2 = read_and_rename(df2_path, 'dbfs:/Metadata/Metacolumns__1_.csv', '4-WheelSales',parquet_path_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ddf81a-da2d-48d3-a69c-f1d0558d930a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, when, to_date\n",
    "from datetime import datetime\n",
    "\n",
    "def date_comparison(srcpath, metapath, filename):\n",
    "    try:\n",
    "        dfs = spark.read.csv(srcpath, header=True, inferSchema=True)\n",
    "        ref_df = spark.read.csv(metapath, header=True, inferSchema=True).withColumn('FILECOLUMNS', trim('FILECOLUMNS'))\n",
    "        ref_filter = ref_df.filter(col('FILENAME') == filename)\n",
    "        date_columns_ref = [row['FILECOLUMNS'] for row in ref_filter.select('FILECOLUMNS').filter(col('FILECOLUMNS').contains('Date')).collect()]\n",
    "        date_columns_dfs = [x for x in dfs.columns if 'Date' in x]\n",
    "\n",
    "        if date_columns_ref:\n",
    "            for date_column in date_columns_ref:\n",
    "                if date_column in date_columns_dfs:\n",
    "                    date_format = 'yyyy-MM-dd'\n",
    "                    dfs = dfs.withColumn('bad_records', when(col(date_column).isNull(), False).when(to_date(col(date_column), date_format).isNotNull(), 'Matched').otherwise('Not Matched'))\n",
    "                else:\n",
    "                    print(f\"No date column found in the {filename}\")\n",
    "\n",
    "            bad_records_df = dfs.filter(col('bad_records') == 'Not Matched')\n",
    "            good_records_df = dfs.filter(col('bad_records') == 'Matched')\n",
    "            dt_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "            output_bad_path = f'dbfs:/BadRecords/{filename}-DateMismatched/{dt_string}'\n",
    "            output_good_path = f'dbfs:/GoodRecords/{filename}/{dt_string}'\n",
    "\n",
    "            if bad_records_df.count() >= 1:\n",
    "                bad_records_df.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").parquet(output_bad_path)\n",
    "            else:\n",
    "                print(\"No bad records found.\")\n",
    "            if good_records_df.count() >= 1:\n",
    "                good_records_df.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").parquet(output_good_path)\n",
    "            else:\n",
    "                print(\"No good records found.\")\n",
    "        else:\n",
    "            print(\"No date columns found in metadata for the given filename.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred while processing date_comparison: {str(e)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5aff2e-727d-44df-bb65-07303a6c71b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No date columns found in metadata for the given filename.\nNo date columns found in metadata for the given filename.\n"
     ]
    }
   ],
   "source": [
    "df1 = date_comparison(df1_path, 'dbfs:/Metadata/Metacolumns__1_.csv', '2-WheelSales')\n",
    "df2 = date_comparison(df2_path, 'dbfs:/Metadata/Metacolumns__1_.csv', '4-WheelSales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c289e5df-23a9-42de-a877-d3762e253a2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [FileInfo(path='dbfs:/BadRecords/', name='BadRecords/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/Bronze/', name='Bronze/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/', name='FileStore/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/Gold/', name='Gold/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/Landing/', name='Landing/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/Metadata/', name='Metadata/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/Silver/', name='Silver/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/databricks-datasets/', name='databricks-datasets/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/databricks-results/', name='databricks-results/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/delta_tables/', name='delta_tables/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/user/', name='user/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "# # dbutils.fs.mkdirs('BadRecords')\n",
    "# dbutils.fs.ls('/')\n",
    "# # path='dbfs:/BadRecords/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9796bd-5a9d-4800-b216-d2aa87f5ac41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2_path 91440\ndf3 count:: 91440\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime \n",
    "# dt_string = datetime.now().strftime(\"%Y/%m/%d\")\n",
    "# # metadata_path = 'dbfs:/Metadata/Metacolumns__1_.csv'\n",
    "# # Metadata = spark.read.csv(metadata_path, header='True')\n",
    "# # df1_path = f\"dbfs:/Bronze/2_WheelSales/{dt_string}\"\n",
    "# df2_path = spark.read.csv(f\"dbfs:/Bronze/4_WheelSales/{dt_string}\", header='True', inferSchema= 'True')\n",
    "\n",
    "# print('df2_path', df2_path.count())\n",
    "\n",
    "# df3 = df2_path.dropDuplicates()\n",
    "# print(\"df3 count::\", df3.count())\n",
    "\n",
    "# # dbfs:/Bronze/4_WheelSales/2024/03/25\n",
    "# # parquet_path_df1 = 'dbfs:/Silver'\n",
    "# # parquet_path_df2 = 'dbfs:/Silver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8de0fae-ea10-485b-87a1-7a392f4ff0e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# column_comparision(df1_path, '2-WheelSales', parquet_path_df1)\n",
    "# column_comparision(df2_path, '4-WheelSales', parquet_path_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f12193d-39f4-4ed5-a97d-8cef9f8e74ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# schema_comparision(df1_path, '2-WheelSales', parquet_path_df1)\n",
    "# schema_comparision(df2_path, '4-WheelSales', parquet_path_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ef2ba9-5221-447d-974b-12c94e1722ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill_nulls_duplicates(df1_path, '2-WheelSales', parquet_path_df1)\n",
    "# fill_nulls_duplicates(df2_path, '4-WheelSales', parquet_path_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b383d7-084f-432e-ad0e-d8b9c9fb7c70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3918072940602230>:2\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    df2 = read_and_rename(df2_path, metadata_path, '4-WheelSales',parquet_path_df2)\u001B[0m\n",
       "\u001B[0m    ^\u001B[0m\n",
       "\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected indent\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-3918072940602230>:2\u001B[0;36m\u001B[0m\n\u001B[0;31m    df2 = read_and_rename(df2_path, metadata_path, '4-WheelSales',parquet_path_df2)\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected indent\n",
       "errorSummary": "<span class='ansi-red-fg'>IndentationError</span>: unexpected indent (<command-3918072940602230>, line 2)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df1 = read_and_rename(df1_path, metadata_path, '2-WheelSales',parquet_path_df1)\n",
    "# df2 = read_and_rename(df2_path, metadata_path, '4-WheelSales',parquet_path_df2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_datavalidation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
