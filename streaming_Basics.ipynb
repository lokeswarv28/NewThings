{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff4e0ff-4e22-4a2e-b388-f264582f8f98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [FileInfo(path='dbfs:/FileStore/03__DBUtis___File_System_Utilities.ipynb', name='03__DBUtis___File_System_Utilities.ipynb', size=37614, modificationTime=1708670803000),\n FileInfo(path='dbfs:/FileStore/BadRecords.csv', name='BadRecords.csv', size=177, modificationTime=1704714188000),\n FileInfo(path='dbfs:/FileStore/Returns_data.csv', name='Returns_data.csv', size=31762, modificationTime=1706092817000),\n FileInfo(path='dbfs:/FileStore/SchemaManagementDelta.csv', name='SchemaManagementDelta.csv', size=101806, modificationTime=1709043986000),\n FileInfo(path='dbfs:/FileStore/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1704167189000),\n FileInfo(path='dbfs:/FileStore/_committed_7407023587918272950', name='_committed_7407023587918272950', size=529, modificationTime=1704167189000),\n FileInfo(path='dbfs:/FileStore/_committed_8527669926312473284', name='_committed_8527669926312473284', size=1210, modificationTime=1704167179000),\n FileInfo(path='dbfs:/FileStore/_started_7407023587918272950', name='_started_7407023587918272950', size=0, modificationTime=1704167188000),\n FileInfo(path='dbfs:/FileStore/_started_8527669926312473284', name='_started_8527669926312473284', size=0, modificationTime=1704167171000),\n FileInfo(path='dbfs:/FileStore/badrecords.csv', name='badrecords.csv', size=157, modificationTime=1704714066000),\n FileInfo(path='dbfs:/FileStore/badrecords.txt', name='badrecords.txt', size=157, modificationTime=1704714016000),\n FileInfo(path='dbfs:/FileStore/corrupted_records_check-1.csv', name='corrupted_records_check-1.csv', size=131, modificationTime=1706083714000),\n FileInfo(path='dbfs:/FileStore/corrupted_records_check.csv', name='corrupted_records_check.csv', size=131, modificationTime=1706083292000),\n FileInfo(path='dbfs:/FileStore/file_example_XLS_10.xls', name='file_example_XLS_10.xls', size=8704, modificationTime=1708438121000),\n FileInfo(path='dbfs:/FileStore/input.csv/', name='input.csv/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/jars/', name='jars/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/part-00000-tid-7407023587918272950-772ef0a1-d50b-47e7-a3bf-fc88bc6b17a1-20-1-c000.snappy.parquet', name='part-00000-tid-7407023587918272950-772ef0a1-d50b-47e7-a3bf-fc88bc6b17a1-20-1-c000.snappy.parquet', size=11995, modificationTime=1704167188000),\n FileInfo(path='dbfs:/FileStore/part_00000_tid_1471089359414595245_b3ec69b3_fbb3_4b3f_a690_c3952983a53e_306_1_c000_snappy.parquet', name='part_00000_tid_1471089359414595245_b3ec69b3_fbb3_4b3f_a690_c3952983a53e_306_1_c000_snappy.parquet', size=10787, modificationTime=1706090172000),\n FileInfo(path='dbfs:/FileStore/part_00000_tid_4324272898605943430_94a5ea7d_7d19_4eb2_a539_d0163ad14b74_200_1_c000_snappy.parquet', name='part_00000_tid_4324272898605943430_94a5ea7d_7d19_4eb2_a539_d0163ad14b74_200_1_c000_snappy.parquet', size=1327, modificationTime=1704712172000),\n FileInfo(path='dbfs:/FileStore/tables/', name='tables/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/testdata.txt', name='testdata.txt', size=105, modificationTime=1705472290000),\n FileInfo(path='dbfs:/FileStore/testdata2-1.csv', name='testdata2-1.csv', size=143, modificationTime=1706166508000),\n FileInfo(path='dbfs:/FileStore/testdata2.csv', name='testdata2.csv', size=113, modificationTime=1706162114000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32860638-d83a-45aa-96d5-628ddba3fb14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[47]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm('dbfs:/FileStore/streaming/', recurse=True)\n",
    "#dbutils.fs.rm('dbfs:/FileStore/CheckPoints/', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e183a2-f4df-43b1-b69d-2e14df29213b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaae2843-e610-47a0-a900-f656607df3a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "ReadStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5067aa4-a12e-4c3d-8cbe-c428023da37c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path1 = 'dbfs:/FileStore/streaming/Countries1.csv'\n",
    "\n",
    "# To read the Stream , allways prefer schema explicitly for streams \n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "            StructField('Country', StringType()),\n",
    "            StructField('Citizens', IntegerType())   \n",
    "])\n",
    "\n",
    "df = spark.readStream.format('csv').option('header', 'True').schema(schema).load(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d94d57f0-d92c-432d-8a91-b38882b04f35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#if you absorve the Spark UI , No job is initiated.. Although we are reading a read stream, it is not going to initiate any job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed01a12-1d84-485a-b392-e4f2fea0154f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-677751719813613>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m    917\u001B[0m     )\n",
       "\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n",
       "\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Queries with streaming sources must be executed with writeStream.start();\n",
       "FileSource[dbfs:/FileStore/streaming/Countries1.csv]"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-677751719813613>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    917\u001B[0m     )\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Queries with streaming sources must be executed with writeStream.start();\nFileSource[dbfs:/FileStore/streaming/Countries1.csv]",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Queries with streaming sources must be executed with writeStream.start();\nFileSource[dbfs:/FileStore/streaming/Countries1.csv]",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.show() # error : Queries with streaming sources must be executed with writeStream.start();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c64013-f959-47c5-bd84-625a85369f85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#use display to show error : Option 'basePath' must be a directory\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39eeda48-614e-4773-8cf0-c18e69a1e9de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## when I'm using the display, it is stating that the base path must be a directory\n",
    "## The reason it is stating is because I am directly referring to a CSV file while reading as a df\n",
    "## But streaming data is something it is going to accept the files under this particular directory.\n",
    "## It cannot just refer to one direct file and it cannot accept the reading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a0dcc28-3f99-42dd-bf56-99288f7500e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_dir = 'dbfs:/FileStore/streaming/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c4e1ebc-0be0-4705-b0aa-7947132424ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_1 = spark.readStream.format('csv').option('Header', 'True').schema(schema).load(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6618868-f890-42d2-8cd0-f963e21b8a92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Country</th><th>Citizens</th></tr></thead><tbody><tr><td>India</td><td>10</td></tr><tr><td>USA</td><td>5</td></tr><tr><td>China</td><td>10</td></tr><tr><td>India</td><td>10</td></tr><tr><td>Canada</td><td>40</td></tr><tr><td>Brazil</td><td>10</td></tr><tr><td>India</td><td>5</td></tr><tr><td>USA</td><td>10</td></tr><tr><td>China</td><td>5</td></tr><tr><td>India</td><td>5</td></tr><tr><td>Canada</td><td>10</td></tr><tr><td>Brazil</td><td>50</td></tr><tr><td>India</td><td>15</td></tr><tr><td>USA</td><td>20</td></tr><tr><td>China</td><td>10</td></tr><tr><td>India</td><td>10</td></tr><tr><td>USA</td><td>5</td></tr><tr><td>China</td><td>10</td></tr><tr><td>India</td><td>10</td></tr><tr><td>Canada</td><td>40</td></tr><tr><td>Brazil</td><td>10</td></tr><tr><td>India</td><td>10</td></tr><tr><td>USA</td><td>10</td></tr><tr><td>China</td><td>20</td></tr><tr><td>Brazil</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "India",
         10
        ],
        [
         "USA",
         5
        ],
        [
         "China",
         10
        ],
        [
         "India",
         10
        ],
        [
         "Canada",
         40
        ],
        [
         "Brazil",
         10
        ],
        [
         "India",
         5
        ],
        [
         "USA",
         10
        ],
        [
         "China",
         5
        ],
        [
         "India",
         5
        ],
        [
         "Canada",
         10
        ],
        [
         "Brazil",
         50
        ],
        [
         "India",
         15
        ],
        [
         "USA",
         20
        ],
        [
         "China",
         10
        ],
        [
         "India",
         10
        ],
        [
         "USA",
         5
        ],
        [
         "China",
         10
        ],
        [
         "India",
         10
        ],
        [
         "Canada",
         40
        ],
        [
         "Brazil",
         10
        ],
        [
         "India",
         10
        ],
        [
         "USA",
         10
        ],
        [
         "China",
         20
        ],
        [
         "Brazil",
         10
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Citizens",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26e8df8f-04b7-4ae8-8eec-17d9a7c27470",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    ">> Spark Streaming is going to invoke a job whenever there is a data available.\n",
    "\n",
    ">> We are not re executing any of the cell, we are just uploading the file and spark is able to identify that there is a file available and it is going to process it, or it is going to display that.\n",
    "\n",
    ">> So when you have a streaming source like a folder which have the streaming files getting added when\n",
    "\n",
    ">> Each file is consider as a micro batch in spark strcutured streaming or Number of files coming as next after one is processing called micro-batch. Ex: If i upload two files for first time then Batch = 1, next time if you upload 3 files Batches = 2 like wise \n",
    "\n",
    ">> Click on Cancel Button to Stop the Structure Streaming \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46fc3d4f-01ef-42f5-948c-3652c2b4f0ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark Streaming Supports different sources and sinks \n",
    "Sources \n",
    ">> File Source, Table, Kafka Source, Socket Sources, Rate Sources\n",
    "\n",
    "Sinks\n",
    ">> File Sink, Kafka Sink, ForeachSink, Console Sink, Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdd62ba8-ade2-44e0-a89f-422a4d0d3631",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Write Stream : To write streaming data to some place \n",
    ">> CheckPointLoaction :  checkpointing it is basically used to store the progress of our stream Like having the metadata till where the data is copied.Which means if I am just telling some directory, if there is a stream that is available, it is going to read that particular stream, and it is going to write the data to a destination, and it is going to note down till where the data is been copied.It is not going to store the data, it is just going to have the metadata till where the point is copied.\n",
    "\n",
    ">> the use of this checkpointing : it is going to give the fault tolerance and resiliency to our streaming data.So to better understand the fault tolerance and the resilient terms, if there is any failure that occurs during the copy of this particular stream, spark is smart enough to start from the point of failure because it is going to store the intermediate metadata in the checkpoints.\n",
    "\n",
    ">> It will go to the checkpoint location, and it is going to see till where the data is copied and going to begin the data copy from there.\n",
    "\n",
    ">> It Must be Unique to Each Stream : Means Have a unique checkpointLoaction for each streaming Query , because one streaming query cannot overlap with the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857bd7c7-39ef-4e30-a515-49d4bf177fa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: [FileInfo(path='dbfs:/FileStore/imgs/structstreaming1.PNG', name='structstreaming1.PNG', size=107582, modificationTime=1709115587000),\n FileInfo(path='dbfs:/FileStore/imgs/structstreamingO.PNG', name='structstreamingO.PNG', size=141822, modificationTime=1709115587000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9477dcd1-8675-4014-a3ce-ffdf20b37b39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "create schema if not exists stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d749d778-de26-4e13-9ec9-b678f0762c48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: True"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7e9436-12a8-429d-80aa-ce707a932d9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f024d0-6db5-4dd0-ab57-152588999fd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " WriteStream = ( df_1.writeStream\n",
    "        .option('checkpointLocation',f'{source_dir}/AppendCheckpoint')\n",
    "        .outputMode(\"append\")\n",
    "        .queryName('AppendQuery')\n",
    "        .toTable(\"stream.AppendTable\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c268c1ae-7278-4802-bd53-556c56897d22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Country</th><th>Citizens</th></tr></thead><tbody><tr><td>India</td><td>10</td></tr><tr><td>USA</td><td>5</td></tr><tr><td>China</td><td>10</td></tr><tr><td>India</td><td>10</td></tr><tr><td>Canada</td><td>40</td></tr><tr><td>Brazil</td><td>10</td></tr><tr><td>India</td><td>5</td></tr><tr><td>USA</td><td>10</td></tr><tr><td>China</td><td>5</td></tr><tr><td>India</td><td>5</td></tr><tr><td>Canada</td><td>10</td></tr><tr><td>Brazil</td><td>50</td></tr><tr><td>India</td><td>10</td></tr><tr><td>USA</td><td>5</td></tr><tr><td>China</td><td>10</td></tr><tr><td>India</td><td>10</td></tr><tr><td>Canada</td><td>40</td></tr><tr><td>Brazil</td><td>10</td></tr><tr><td>India</td><td>10</td></tr><tr><td>USA</td><td>10</td></tr><tr><td>China</td><td>20</td></tr><tr><td>Brazil</td><td>10</td></tr><tr><td>India</td><td>15</td></tr><tr><td>USA</td><td>20</td></tr><tr><td>China</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "India",
         10
        ],
        [
         "USA",
         5
        ],
        [
         "China",
         10
        ],
        [
         "India",
         10
        ],
        [
         "Canada",
         40
        ],
        [
         "Brazil",
         10
        ],
        [
         "India",
         5
        ],
        [
         "USA",
         10
        ],
        [
         "China",
         5
        ],
        [
         "India",
         5
        ],
        [
         "Canada",
         10
        ],
        [
         "Brazil",
         50
        ],
        [
         "India",
         10
        ],
        [
         "USA",
         5
        ],
        [
         "China",
         10
        ],
        [
         "India",
         10
        ],
        [
         "Canada",
         40
        ],
        [
         "Brazil",
         10
        ],
        [
         "India",
         10
        ],
        [
         "USA",
         10
        ],
        [
         "China",
         20
        ],
        [
         "Brazil",
         10
        ],
        [
         "India",
         15
        ],
        [
         "USA",
         20
        ],
        [
         "China",
         10
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Citizens",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "select * from stream.AppendTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "832beb6d-e0b5-4d5b-8c3d-03e78c743a00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##STOP WRITESTREAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1217114-ad71-48ff-98c0-593e8f41cc6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WriteStream.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 677751719813628,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "streaming_Basics",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
