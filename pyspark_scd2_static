from pyspark.sql.functions import *
from pyspark.sql.types import * 

# dbutils.fs.ls('/user/hive/warehouse/')

try:
    dir_path = dbutils.fs.ls('/user/hive/warehouse/dev.db')

    if dir_path:
        print('Presented and trying to remove')

        dbutils.fs.rm('dbfs:/user/hive/warehouse/dev.db/', True)

        print('removed')

    else : 
        print("An empty dir found, please check the path")

except Exception as e:
    print(f"An error occured while processing this code : {str(e)}")


%sql

create schema if not exists dev;

%sql

create  table if not exists dev.DimEmployees2 (
  employee_key BIGINT GENERATED ALWAYS AS IDENTITY,
  employee_id int ,
  employee_fname string , 
  employee_lname string,
  city string,
  eff_startdate timestamp,
  eff_enddate timestamp

);

%sql
CREATE table if not exists uat.DimModel(
  pkModelId  LONG GENERATED ALWAYS AS IDENTITY,
  ModelName STRING,
  Segment STRING,
  FuelCapacity STRING,
  SeatCapacity STRING,
  EngineCapacity STRING,
  WheelerKey STRING,
  Active_status STRING,
  StartDate TIMESTAMP,
  EndDate TIMESTAMP
);

stg_df_1 = spark.read.format('csv').option('Header', 'True').option('inferSchema', 'True').load('dbfs:/FileStore/testing/2_test.csv').withColumn('FuelCapacity', lit(None))\
           .withColumn('SeatCapacity', lit(None)).select(F.col('Model'), F.col('Segment'), F.col('EngineCapacity'), F.col('FuelCapacity'), F.col('SeatCapacity'), F.lit('T').alias("wheelerkey")).dropDuplicates(['Model', 'Segment', 'EngineCapacity'])

stg_df_2 = spark.read.format('csv').option('Header', 'True').option('inferSchema', 'True').load('dbfs:/FileStore/testing/4_test.csv').select(F.col('Model'), F.col('Body Type').alias('Segment'), F.col('ENGINE').alias('EngineCapacity'), F.col('Fuel Capacity (L)').alias('FuelCapacity'), F.col('Seating Capacity').alias('SeatCapacity') , F.lit('F').alias("wheelerkey") ).dropDuplicates(['Model', 'Segment', 'EngineCapacity'])


print('stg_df_1 count:', stg_df_1.count())
print('stg_df_2 count', stg_df_2.count())


#dbfs:/FileStore/2-WheelSales/2_WheelSales.csv
# print("Source_data")

# stg_df = stg_df.dropDuplicates(['Model', 'Segment', 'EngineCapacity'])

stg_df = stg_df_1.unionByName(stg_df_2)

print('stg_df count:', stg_df.count())

stg_df.show()

stg_df = stg_df.withColumnRenamed("Model", "ModelName")

print('read the table')

dim_df = spark.sql('select * from uat.dimmodel')

dim_df.show()

print('Identify New Records: Not present in dim_df')

new_records_df = stg_df.join(dim_df, ['ModelName'], "left_anti").select(stg_df['*'], F.current_date().alias('StartDate'), F.lit('9999-12-31').alias('EndDate'), F.lit(None).alias('pkmodelid'))

print('new_records_df count', new_records_df.count())

new_records_df.show()


update_df = dim_df.join(stg_df, stg_df['ModelName'] == dim_df['ModelName'], 'inner') \
            .filter((dim_df['EngineCapacity'] != stg_df['EngineCapacity']))\
            .filter(dim_df['EndDate'] == '9999-12-31') \
            .select(dim_df['*']).withColumn('EndDate', current_date()).dropDuplicates(['ModelName', 'Segment', 'EngineCapacity'])
print('update_df')

update_df.show()

print("Identify Changed Records: Exists in dim_df but with different EngineCapacity ")

updated_records_df = stg_df.join(
    update_df.select("ModelName"),
    stg_df['ModelName'] == dim_df['ModelName'],
    how="inner"
).select(
    stg_df['*'],
    F.current_date().alias("StartDate"),
    F.lit("9999-12-31").alias("EndDate")
).dropDuplicates(['ModelName', 'Segment', 'EngineCapacity'])

print('updated_records_df count::', updated_records_df.count())

# updated_records_df.show()

for col in [col.lower() for col in dim_df.columns if col not in new_records_df.columns]:
    new_df = new_records_df.withColumn(col, lit(None))

for col in [col.lower() for col in dim_df.columns if col not in updated_records_df.columns]:
    updated_records_df = updated_records_df.withColumn(col, lit(None))


print("New_df display")
new_df.show()

print('updated_records_df')
updated_records_df.show()

print("merge the update dataframes")

merge_updates = updated_records_df.unionByName(update_df)

merge_updates.show()

print("merging the updates and new dataframes")
merged_df = merge_updates.unionByName(new_df).dropDuplicates(['ModelName', 'Segment', 'EngineCapacity']).drop('pkmodelid')
print('merge count:', merged_df.count())
# merged_df.show()

window_spec = Window.partitionBy("ModelName", "Segment", "EngineCapacity", "FuelCapacity", "SeatCapacity", "WheelerKey").orderBy("StartDate")


final_df = merged_df.withColumn('StartDate', to_timestamp('StartDate')).withColumn('EndDate', to_timestamp('EndDate')).withColumn('FuelCapacity',  F.col('FuelCapacity').cast(StringType())).withColumn('SeatCapacity',  F.col('SeatCapacity').cast(StringType()))

final_df = final_df.dropDuplicates(['ModelName', 'Segment', 'EngineCapacity'])
final_df.show()

print('final_df count::', final_df.count())

print('add column to final_df ')

final_df_2 = final_df.withColumn('pkModelId', lit(None))

union_df = final_df_2.unionByName(dim_df) 

union_df = union_df.withColumn("row_num", row_number().over(window_spec))

# Filter out old records (row_num > 1)
dim_df_updated = union_df.filter("row_num = 1")

print("dim_df_updated count::", dim_df_updated.count())

dim_df_updated = dim_df_updated.drop('row_num', 'pkModelId')

print('final df writing to table')
print('1...2....3...')
if dim_df_updated.count() > 0:

    print("founded new or updated records.. overwriting the table")
    dim_df_updated.write.mode('overwrite').saveAsTable('uat.dimmodel')
    print("success....")
else:
    print("No new or updates founded...")


========================= ================================================= SCD1 ====================================== ===========


create table if not exists gold.dimbrand2 (
    brand_id Long GENERATED ALWAYS AS IDENTITY,
    BrandName string,
    Segment string

);

stg_df = spark.read.format('csv').option('Header', 'True').option('inferSchema', 'True').load('dbfs:/FileStore/barnd_file-6.csv')

stg_df.show()

dim_df = spark.sql("""SELECT * FROM  gold.dimbrand2 """)

# dim_df = dim_df.withColumn("tempKey", monotonically_increasing_id())

dim_df.show()

print("check for new records...")

new_records = stg_df.join(dim_df, 'BrandName', 'left_anti')
new_records.show()

updated_records = stg_df.join(dim_df, stg_df['BrandName'] == dim_df['BrandName'], 'inner').select(dim_df['*'])

print("matching from src to target ")
updated_records.show()

combined_records = new_records.unionByName(updated_records.drop('brand_id'))

print('combined....')

combined_records.show()

# new_records = new_records.drop('name_md5')

combined_records.write.mode('overwrite').saveAsTable("gold.dimbrand2")


=========================================================================================================================

# Function to generate or load a DataFrame
def generate_df():
    # Your code to generate or load the DataFrame
    # For example:
    df = spark.createDataFrame([(1, 'Alice'), (2, 'Bob'), (3, 'Charlie')], ['id', 'name'])
    return df

# Function to operate on the DataFrame returned by generate_df()
def process_df(df):
    # Your code to process the DataFrame
    # For example:
    df_processed = df.withColumn('name_length', F.length(df['name']))
    return df_processed

# Call generate_df() to get the DataFrame
df = generate_df()

# Call process_df() with the returned DataFrame
processed_df = process_df(df)

# Display the processed DataFrame
display(processed_df)


