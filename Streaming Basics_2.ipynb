{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24642a5-a1dc-4349-9fb5-b516a31cb564",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    ">> In community edition if our cluster terminates , we can't access the schema or DB or Table where we created with previous cluster \n",
    "\n",
    ">> The schema completely lost \n",
    "\n",
    ">> So lets make use of following script which will create the DataBase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b603ab-82ee-4110-8360-4e06faadeeaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: [FileInfo(path='dbfs:/user/hive/warehouse/stream.db/appendtable/', name='appendtable/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/user/hive/warehouse/stream.db/table1/', name='table1/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "#dbutils.fs.ls('dbfs:/user/hive/warehouse/stream.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "509e527b-692e-4a88-bcf5-423aed89e9b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    ">> when I'm running this script tomorrow it is going to delete everything in this particular folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52916364-9f4a-455a-af2a-8de500a58c6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/stream.db', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f378fea-c2f1-49c4-a05d-d4ff35656e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    ">> So when I'm running this script tomorrow, it is going to drop this particular database and it is going drop If there are any active databases and it is going to recreate this database\n",
    "\n",
    ">> CASCADE\n",
    "\n",
    "If specified, will drop all the associated tables and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13fbbac3-3892-4e5b-a042-add07ac0f478",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "Drop DataBase if exists stream CASCADE;\n",
    "create DataBase if not exists stream "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35fe2372-9314-400b-8f94-fa8299ebc0f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Reprocess the previous data,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb12d957-b145-4234-8c71-cfab276191b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    ">>  in order to reprocess the previous data, you also need to delete the checkpoint location.\n",
    "\n",
    ">> Now if I'm just trying to reprocess this entire data, it is not going to work because there is a checkpoint location where it's # mentioned that this data is already processed.\n",
    "\n",
    ">> These Point we have to keep in our mind , especially in the community edition cluster. once cluster got Terminated and Need to create another cluster. While Using New Cluster we need to delete the previous check point at time of writeStream.\n",
    "\n",
    ">> if you write this query select * from schema.Table we will get No results , beacuse of previous checkpoint says it's processed . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61078a31-2d92-4c7e-b0b0-f1acb1461f1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## empty checkpoint location \n",
    "\n",
    "#dbutils.fs.rm('dbfs:/FileStore/streaming/', recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89e2a781-f15b-4b9a-b457-dbf5bf3575cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Final Steps : Follow These Steps to get the result back which are created in the old cluster with NewCluster \n",
    "\n",
    ">> dbutils.fs.rm('dbfs:/user/hive/warehouse/stream.db', True)\n",
    "\n",
    ">> dbutils.fs.rm('dbfs:/FileStore/streaming/', recurse=True)\n",
    "\n",
    ">> %sql \n",
    "Drop DataBase if exists stream CASCADE;\n",
    "create DataBase if not exists stream \n",
    "\n",
    ">> Code for ReadStream \n",
    "\n",
    ">> code for writeStream with NewCheckPoint Location \n",
    " WriteStream = ( df_1.writeStream\n",
    "        .option('checkpointLocation',f'{source_dir}/AppendCheckpointNew')\n",
    "        .outputMode(\"append\")\n",
    "        .queryName('AppendQuery')\n",
    "        .toTable(\"stream.AppendTable\"))\n",
    "\n",
    ">> %sql select * from stream.AppendTable"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming Basics_2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
